{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJET DEEP LEARNING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTATION DES MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE-TRAITEMENT DES DONNEES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training damage images:  5000\n",
      "total validation damage images:  1000\n",
      "total test damage images:  8000\n",
      "total training no damage images:  5000\n",
      "total validation no damage images:  1000\n",
      "total test no damage images:  1000\n"
     ]
    }
   ],
   "source": [
    "print('total training damage images: ',len(os.listdir(\"./train_another/damage\")))\n",
    "print('total validation damage images: ',len(os.listdir(\"./validation_another/damage\")))\n",
    "print('total test damage images: ',len(os.listdir(\"./test_another/damage\")))\n",
    "print('total training no damage images: ',len(os.listdir(\"./train_another/no_damage\")))\n",
    "print('total validation no damage images: ',len(os.listdir(\"./validation_another/no_damage\")))\n",
    "print('total test no damage images: ',len(os.listdir(\"./test_another/no_damage\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des images en séparant en jeu train / test / validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((150,150)),\n",
    "                                 transforms.ToTensor()])\n",
    "batch_size = 20\n",
    "\n",
    "df_train = datasets.ImageFolder(\"./train_another\",transform=transform)\n",
    "train_dataloader = DataLoader(dataset=df_train, batch_size=batch_size, shuffle=True, num_workers = 4 , pin_memory = True)\n",
    "\n",
    "df_test = datasets.ImageFolder(\"./test_another\",transform=transform)\n",
    "test_dataloader = DataLoader(dataset=df_test, batch_size=batch_size, shuffle=True,num_workers = 4 , pin_memory = True)\n",
    "\n",
    "df_validation = datasets.ImageFolder(\"./validation_another\",transform=transform)\n",
    "validation_dataloader = DataLoader(dataset=df_validation, batch_size=batch_size, shuffle=True,num_workers = 4 , pin_memory = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons de visualiser les classes de nos images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['damage', 'no_damage']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation.classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELISATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELE 1 : Reproduction du modèle de CAO & CHOE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de la classe qui va nous permettre de faire notre classification d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de la classe qui va permettre de créer notre premier modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HurricaneClassification(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(3,32, kernel_size = (3,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(32,64, kernel_size = (3,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        \n",
    "            nn.Conv2d(64, 128, kernel_size = (3,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(128,128, kernel_size = (3,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(6272, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création des fonctions qui vont permettre d'évaluer notre modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "  \n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    nb_etape = 0\n",
    "    outputs = []\n",
    "    for batch in val_loader:\n",
    "        nb_etape = nb_etape + 1\n",
    "        outputs.append(model.validation_step(batch))\n",
    "        if nb_etape == 50:\n",
    "            break\n",
    "            \n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "  \n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    \n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        nb_etape = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            nb_etape = nb_etape+1\n",
    "            if nb_etape == 10:\n",
    "                break\n",
    "                \n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    \n",
    "    return history, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 0.6948, val_loss: 0.6929, val_acc: 0.5090\n",
      "Epoch [1], train_loss: 0.6928, val_loss: 0.6926, val_acc: 0.5020\n",
      "Epoch [2], train_loss: 0.6928, val_loss: 0.6919, val_acc: 0.4880\n",
      "Epoch [3], train_loss: 0.6928, val_loss: 0.6929, val_acc: 0.5030\n",
      "Epoch [4], train_loss: 0.6923, val_loss: 0.6901, val_acc: 0.5040\n"
     ]
    }
   ],
   "source": [
    "model = HurricaneClassification()\n",
    "num_epochs = 5\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 1e-4\n",
    "#fitting the model on training data and record the result after each epoch\n",
    "history, loss = fit(num_epochs, lr, model, train_dataloader, validation_dataloader, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 148, 148]             896\n",
      "              ReLU-2         [-1, 32, 148, 148]               0\n",
      "         MaxPool2d-3           [-1, 32, 74, 74]               0\n",
      "            Conv2d-4           [-1, 64, 72, 72]          18,496\n",
      "              ReLU-5           [-1, 64, 72, 72]               0\n",
      "         MaxPool2d-6           [-1, 64, 36, 36]               0\n",
      "            Conv2d-7          [-1, 128, 34, 34]          73,856\n",
      "              ReLU-8          [-1, 128, 34, 34]               0\n",
      "         MaxPool2d-9          [-1, 128, 17, 17]               0\n",
      "           Conv2d-10          [-1, 128, 15, 15]         147,584\n",
      "             ReLU-11          [-1, 128, 15, 15]               0\n",
      "        MaxPool2d-12            [-1, 128, 7, 7]               0\n",
      "          Flatten-13                 [-1, 6272]               0\n",
      "          Dropout-14                 [-1, 6272]               0\n",
      "           Linear-15                  [-1, 512]       3,211,776\n",
      "             ReLU-16                  [-1, 512]               0\n",
      "           Linear-17                    [-1, 2]           1,026\n",
      "             ReLU-18                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 3,453,634\n",
      "Trainable params: 3,453,634\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.26\n",
      "Forward/backward pass size (MB): 20.86\n",
      "Params size (MB): 13.17\n",
      "Estimated Total Size (MB): 34.29\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(3, 150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe Kernel s’est bloqué lors de l’exécution du code dans la cellule active ou une cellule précédente. Veuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. Cliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. Pour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "plt.plot(loss)\n",
    "plt.xlabel('Itération')\n",
    "plt.ylabel('Perte')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43846aba58453b3fc7a3ceda9f4997b299ab26354919d73b0d9b0c9dbd4441fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
